---
title: "Final Exam"
author: "Arun Ram Sankaranarayanan"
date: "December 14, 2015"
output: html_document
---

1) The five main Rs of EDA are 
1. Resistance 
2. Residuals 
3. Re-expression 
4. Revelations 
5. Reiteration



2)


There are two functions that are used to calculate the five number summary in R
1.fivenum()
2.Summary()

```{r,echo=TRUE}

dat = seq(1, 9, by = 2)

fivenum(dat)

summary(dat)

```


3)
1. TO make the distribution more symmetric - We can easily summarice the center of the distribution that way
2. To make spread of several groups more alike - That way we can compare groups that share common spread
3.To transform the data to normality
4. Detect outliers


4)


Generate a tailed distribution

```{r,echo=TRUE}

population1<-rgamma(1000,shape=2)
par(mfrow=c(1,1),mfcol=c(1,2))

qqnorm(population1)
plot(density(population1))

population1.trans<-(population1)^1/3

qqnorm(population1.trans)
plot(density(population1.trans))


```
From the qq plot we can see the long tailness of data also with the help of gh estimators the heavy tailness can be detected 



5)

For g-H distributions

when g=0, h=0 Gaussian Data #No skewness #No Long Tails
when g<0.25,h>0 slight skewness with Long Tail
when gâ¼1,h>0 very skewed with Long tail


The given gh estimates are (-0.5,0.3) , (0.5,0.3), (1,0.6)

(g,h) -> (-0.5,0.3) is left skewed and has moderate tails
(g,h) -> (0.5,0.3)  is right skewed and has moderate tails
(g,h) -> (1,0.6) is  highly right skewed and has heavy tails



6)

Given

```{r,echo=TRUE}
a=c(1092,1137,1197,1237,1301,1523,1577,1619,1626,1644,
    1672,1748,1768,1780,1796,1816,1843,1844,1902,1919,
    1993,2025,2028,2032,2036,2072,2078,2090,2137,2162,
    2163,2180,2185,2194,2225,2230,2233,2234,2235,2265,
    2270,2274,2281,2289,2319,2322,2357,2381,2398,2421,
    2421,2443,2522,2549,2552,2581,2618,2618,2620,2624,
    2642,2647,2666,2705, 2721,2740,2804,2819,2823,2860,
    2873,2906,2913,2926,2929,2931,2931,2934,2939,2961,3020,3023,3044,3047,
    3048,3096,3174,3190,3199,3204,3222,3225,3278,3287,
    3292,3300,3339,3361,3412,3462,3503,3530,3589,3672,
    3734,3749,3783,3854,3901,3932,3995,4001,4006,4118,
    4134,4320,4346,4385,4401,4522,4565,4581,4593,4629,
    4855,4868,4878,4885,4907,4962,4975,5021,5127,5155,
    5160,5183,5229,5242,5379,5383,5513,5555,5619,5755,
    5774,5890,5899,5988,6161,6185,6818,7406,7419,8175,
    8220,8282,8827,9027,9042,9805)


#a. Construct a QQ plot

plot.new()
par(mfrow=c(1,2))


#Constructing Histogram
hist(a,main="Histogram of given")

#constructing qq plot


qqnorm(a, main="qqplot of given")





#b use g distribution and estimate A,B,g for these data 



getwd()

source("lvalprogs.r")



#lettevalueplot
lvp<-lval(a)

n<-length(a)

print(lvp)

a1 <- 1/2^(1:9)
g1 <- abs(qnorm(a1)) 
a2 <- (lvp[,1]-1/3)/(n + 1/3)
g2 <- abs(qnorm(a2)) 

e2.g <- log((lvp[,3] - lvp[1,2])/(lvp[1,2]-lvp[,2]))/g2


plot(1:(dim(lvp)[1]-1), e2.g[-1],
     xlab="Letter value(1=F, 2=E, 3=D, ... , 8=Y)",
     ylab="g estimate")
abline(h=median(e2.g[-1]))



#g estimate

e.g <- median(e2.g[-1])


cat("Estimate of g:",e.g)



#estimate a and b


source ("rrline.r")


pol <- c(rev(qnorm(a2)),abs(qnorm(a2)))

e.Y <- (exp(e.g*pol)-1)/e.g

y<-c(rev(lvp[,2]),lvp[,3])
plot(e.Y,y)


rline<-run.rrline(e.Y,y)

abline(rline$a, rline$b)


cat("G estimate =",e.g, ",A estimate=" , rline$a, " and B estimate =" ,rline$b)



#c Bootstrap estimates


library(ggplot2)
library(GGally)
g.dist.estimates<-function(sample.pop){
  source("lvalprogs.r")
  source("rrline.r")
  ll<-lval(sample.pop)
  pp1 <- 1/2^(1:nrow(ll)-1)
  gau1 <- abs(qnorm(pp1))
  pp2 <- abs((pp1-1/3)/(nrow(ll)-1 + 1/3))
  gau2 <- abs(qnorm(abs(pp2)))
  est2.g <- log((ll[,3] - ll[1,2])/(ll[1,2]-ll[,2]))/gau2
  
  # Estimation of g
  est.g <- median(est2.g[-1]) 
  p <- c(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99, 0.995)
  zp <- qnorm(p)
  est.Y <- (exp(est.g*zp)-1)/est.g
  rr <- run.rrline(est.Y,quantile(sample.pop,p))
  #Run Resistant Regression for A and B Estimates
  return (list(g=est.g,A=rr$a,B=rr$b))
}


bootstrap.g<-function(pops,sims){
  g.est <- c()
  A.est <- c()
  B.est <- c()
  for (i in 1:sims){
    boot.sample<-sample(pops,length(pops),replace = TRUE)
    r.val <- g.dist.estimates(boot.sample)
    g.est[i]<-r.val$g
    A.est[i]<-r.val$A
    B.est[i]<-r.val$B
  }
  best.g <- mean(g.est)
  g.lower <- best.g - qt(0.975,df=length(g.est)-1)*sd(g.est)
  g.upper <- best.g + qt(0.975,df=length(g.est)-1)*sd(g.est)
  best.A <- mean(A.est)
  A.lower <- best.A - qt(0.975,df=length(A.est)-1)*sd(A.est)
  A.upper <- best.A + qt(0.975,df=length(A.est)-1)*sd(A.est)
  best.B <- mean(B.est)
  B.lower <- best.B - qt(0.975,df=length(B.est)-1)*sd(B.est)
  B.upper <- best.B + qt(0.975,df=length(B.est)-1)*sd(B.est)
  cor.est <- cor (cbind(g.est,A.est,B.est))
  es.plt <- pairs(as.data.frame(cbind(g.est,A.est,B.est)))
  return(list(g=best.g,a=best.A,b=best.B,g.lower,g.upper,A.lower,A.upper,B.lower,B.upper,cor.est,es.plt))
}
a<-c(1092,1137,1197,1237,1301,1523,1577,1619,1626,1644,1672,1748,1768,1780,1796,1816,1843,1844,1902,1919,1983,1993,2025,2028,2032,2036,2072,2078,2090,2137,2162,2163,2180,2185,2194,2225,2230,2233,2234,2235,2265,2270,2274,2281,2289,2319,2322,2357,2381,2398,2421,2421,2443,2522,2549,2552,2581,2618,2618,2620,2624,2642,2647,2666,2705,2721,2740,2804,2819,2823,2860,2873,2906,2913,2926,2929,2931,2931,2934,2939,2961,3020,3023,3044,3047,3048,3096,3174,3190,3199,3204,3222,3225,3278,3287,3292,3300,3339,3361,3412,3462,3503,3530,3589,3672,3734,3749,3783,3854,3901,3932,3995,4001,4006,4118,4134,4320,4346,4385,4401,4522,4565,4581,4593,4629,4855,4868,4878,4885,4907,4962,4975,5021,5127,5155,5160,5183,5229,5242,5379,5383,5513,5555,5619,5755,5774,5890,5899,5988,6161,6185,6818,7406,7419,8175,8220,8282,8827,9027,9042,9805)
bs.val <- bootstrap.g(a,1000)

print(paste("The g Estimate is ",bs.val[1]))
print(paste(" and Confidence interval is between",bs.val[4]," and ",bs.val[5]))
print(paste("The A Estimate is ",bs.val[2]))
print(paste(" and Confidence interval is between",bs.val[6]," and ",bs.val[7]))
print(paste("The B Estimate is ",bs.val[3]))
print(paste(" and Confidence interval is between",bs.val[8]," and ",bs.val[9]))

print(bs.val[10])
print(bs.val[11])

g<-bs.val$g
A<-bs.val$a
B<-bs.val$b



#d


z<- 1/g*log(((a-A)*g)/B +1)
par(mfrow=c(1,2))
hist(z)
qqnorm(z)




# The transformed data is still skewness in the left tail


#e

gof.pearson=function (x,nbins) {
  n = length(x) 
  m = floor(n/nbins) 
  k = n - m*nbins # This is the remainder 
  xx=sort(x) 
  index = rep(1:nbins,m)
  if(k >0){ d=sample(1:nbins,k,replace=FALSE); 
  index=c(index,d) } 
  bincount=as.numeric(table(index)) 
  binindicies = cumsum(bincount) 
  binbreaks = rev(rev(xx[binindicies])[-1]) 
  binbreaks = c(-Inf,binbreaks,Inf) 
  bins=cut(x,breaks=binbreaks)
  internalbreaks = rev(rev(xx[binindicies])[-1]) 
  p = pnorm(internalbreaks,mean(x),sd(x))
  p = c(p[1],diff(p),1-pnorm(max(internalbreaks),mean(x),sd(x)))
  exp = n*p
  df = data.frame(bin=levels(bins),bincount=bincount,prob=p,expectedcount=exp)
  chisqstat = sum((bincount - exp)^2/exp)
  pval = 1- pchisq(chisqstat,nbins-1)
  output = list(df=df,chisq=chisqstat,pval=pval)
  output = list(df=df,chisq=chisqstat,pval=pval)
}
out.p<-gof.pearson(z,2*sqrt(length(z))) #usinf Velleman rule
out.p

library("goftest")
#Correlation of the QQ Data test
qqnorm(z)
qqline(z)

#Shapiro Wilk's Test
shapiro.test(z)
#Anderson-Darling Test
ad.test(z,"pnorm")


#Kolmogorov Test
ks.test(z,"pnorm")
#Cramer-von-Mises Test
cvm.test(z,"pnorm")




```


Shapiro Wilkbs Test -The null-hypothesis of this test is that
 An issue with the Shapiro-Wilkbs test is that when feed with more data, 
the chances of the null hypothesis being rejected becomes larger.
So what happens is that for large amounts of data even very small deviations
from normality can be detected, leading to rejection of the null hypothesis 
event though for practical purposes the data is more than normal enough

ECDF Based Test Statistics
-Empirical testing has found[5] that the Anderson-Darling test is not quite as good as Shapiro Wilk test

Pearson goodness of Fit Test
- A sample with a sufficiently large size is assumed
- If a Pearson Goodness of fit test is conducted on a sample with a smaller size, then the Pearson Goodness of fit test will yield an inaccurate inference -The observations are always assumed to be independent of each other

QQPlot -Easiest to Interpret for skewness and Heavy/Light tails and normality -The Q-Q plot doesnbt give a strong indication of non-normality


7)
```{r,echo=TRUE}


library(boot)


b =c(12.87,15.09,17.39,18.62,20.24,23.76,24.35, 24.74,24.81,24.96,25.19,25.75,25.89,25.97, 26.07,26.19,26.35,26.36,26.67,26.76,27.07, 27.12,27.26,27.28,27.30,27.31,27.46,27.49, 27.54,27.72,27.81,27.82,27.88,27.90,27.93, 28.03,28.05,28.06,28.07,28.07,28.17,28.19, 28.20,28.22,28.25,28.34,28.35,28.46,28.53,28.58,28.64,28.65,28.70,28.92,28.99,29.00, 29.07,29.16,29.16,29.17,29.18,29.22,29.23, 29.28,29.37,29.40,29.45,29.59,29.62,29.63, 29.71,29.74,29.81,29.82,29.85,29.86,29.86, 29.86,29.87,29.88,29.92,30.04,30.05,30.09, 30.09,30.10,30.19,30.34,30.37,30.38,30.39, 30.43,30.43,30.53,30.55,30.55,30.57,30.64, 30.68,30.77,30.86,30.93,30.98,31.08,31.22, 31.32,31.35,31.41,31.52,31.60,31.65,31.76, 31.76,31.77,31.96,31.98,32.28,32.33,32.39, 32.42,32.61,32.68,32.71,32.73,32.79,33.15, 33.18,33.19,33.20,33.24,33.33,33.35,33.43, 33.60,33.65,33.66,33.70,33.77,33.80,34.03, 34.03,34.26,34.33,34.44,34.68,34.71,34.91, 34.93,35.09,35.40,35.44,36.63,37.81,37.84, 39.47,39.58,39.72,41.00,41.49,41.52,43.50)

par(mfrow=c(1,2),mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
hist(b,prob=TRUE)
lines(density(b),col="blue")
qqnorm(b)

ll <- lval(b); 
#ll
n<-length(b)
gh2.data <- b
ll.gh2 <- lval(gh2.data)
yy.gh2 <- log(ll.gh2[-1,6])
xx.gh2 <- (qnorm((ll.gh2[-1,1] - 1/3)/(161 + 1/3)))^2/2 
plot(xx.gh2,yy.gh2,main="Estimate h and B", 
     ylab="log(pseudo-sigma)", xlab=expression(z[p]^2/2),
     sub="rrline: 2.71 + 0.24x => B = 2.71, h = 0.24")
rr <- run.rrline(xx.gh2,yy.gh2);

abline(rr$a, rr$b)

cat( "A=", median(b), ",B=" ,exp(rr$a) , ",h=" ,rr$b)


est.h<-function(b){
  source("lvalprogs.r")
  source("rrline.r")
  ll <- lval(b) 
  n<-length(b)
  gh2.data <- b
  ll.gh2 <- lval(gh2.data)
  yy.gh2 <- log(ll.gh2[-1,6])
  xx.gh2 <- (qnorm((ll.gh2[-1,1] - 1/3)/(161 + 1/3)))^2/2 
  plot(xx.gh2,yy.gh2,main="Estimate h and B", 
       ylab="log(pseudo-sigma)", xlab=expression(z[p]^2/2))
  rr <- run.rrline(xx.gh2,yy.gh2)
  abline(rr$a, rr$b)
  return(list(h=rr$b,A=median(b),B=exp(rr$a)))
}
normal.h<-est.h(b)
print (paste("H estimate is",normal.h$h))
print (paste("A estimate is",normal.h$A))
print (paste("B estimate is",normal.h$B))


est.h<-function(b){
  source("lvalprogs.r")
  source("rrline.r")
  ll <- lval(b) 
  n<-length(b)
  gh2.data <- b
  ll.gh2 <- lval(gh2.data)
  yy.gh2 <- log(ll.gh2[-1,6])
  xx.gh2 <- (qnorm((ll.gh2[-1,1] - 1/3)/(161 + 1/3)))^2/2 
  #plot(xx.gh2,yy.gh2,main="Estimate h and B", 
  #  ylab="log(pseudo-sigma)", xlab=expression(z[p]^2/2))
  rr <- run.rrline(xx.gh2,yy.gh2)
  
  return(list(h=rr$b,A=median(b),B=exp(rr$a)))
}

bootstrap.h<-function(pop,sims){
  library(GGally)
  est.h <-c()
  est.A <-c()
  est.B <-c()
  for (i in 1:sims){
    b<-sample(pop,length(pop),replace = TRUE)
    b.sample <- est.h(b)
    est.h[i]<- b.sample$h
    est.A[i]<- b.sample$A
    est.B[i]<- b.sample$B
  }
  best.h <- mean(est.h)
  g.lower <- best.h - qt(0.9,df=length(est.h)-1)*sd(est.h)
  g.upper <- best.h + qt(0.9,df=length(est.h)-1)*sd(est.h)
  best.A <- mean(est.A)
  A.lower <- best.A - qt(0.9,df=length(est.A)-1)*sd(est.A)
  A.upper <- best.A + qt(0.9,df=length(est.A)-1)*sd(est.A)
  best.B <- mean(est.B)
  B.lower <- best.B - qt(0.9,df=length(est.B)-1)*sd(est.B)
  B.upper <- best.B + qt(0.9,df=length(est.B)-1)*sd(est.B)
  cor.est <- cor (cbind(est.h,est.A,est.B))
  es.plt <- ggpairs(as.data.frame(cbind(est.h,est.A,est.B)))
  return(list(h=best.h,A=best.A,B=best.B,gcil=g.lower,gciu=g.upper,Acil=A.lower,Aciu=A.upper,bcil=B.lower,bciu=B.upper,cor.est=cor.est,es.plt=es.plt))
}
s <- bootstrap.h(b,1000)
cat("The A estimate is ",s$A,",C.I is between",s$Acil," and ",s$Aciu )
cat("The B estimate is ",s$B, ",C.I is between",s$bcil," and ",s$bciu)

cat("The H estimate is ",s$h,",C.I is between",s$gcil," and ",s$gciu)

#Co-Relation Pairs

s$cor.est

#Pairs Plot
print(s$es.plt)





```




8)
```{r,echo=TRUE}


HDistBackXform=function(h,A,B,data){
  ###################################
  #  This function will allow you to back solve for Z
  #  under any H-distribution transform
  #  the Values h, A, and B are the estimated values of
  #  the H-distribution parameters.  In this program
  #  data is a vector of data.
  ###################################
  
  n=length(data)
  #using Veleman's rule
  output=numeric(n)
  g=function(z){z*exp(h*z^2)-((x-A)/B)}
  # Begin loop on i where data[i] is the ith data value
  for(i in 1:n){
    x=data[i]
    obj=uniroot(g,interval=c(-6,6))
    output[i]=obj$root
  }
  return(output)
}

h<-normal.h$h
A<-normal.h$A
B<-normal.h$B

z<-HDistBackXform(h,A,B,b)
par(mfrow=c(1,2),mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
hist(z,prob=TRUE)
lines(density(z),col="blue")
qqnorm(z)
qqline(z,col="red")

#Veleman's Rule for Pearson's GoF

noofbins=2*sqrt(length(z))
out<-gof.pearson(z,noofbins)
out

#9 Estimation of Mode


data = rnorm(100, 3, 2)

getGaussianMax = function(data){
  d = density(data, kernel="gaussian")
  index = which(d$y == max(d$y), arr.ind =TRUE)
  ans = d$x[index]
  return(ans)
}

calculatePseudoValues = function(data) {
  n = length(data)
  y.all = getGaussianMax(data)
  PV = numeric(n)
  for( i in 1:n) {
    yminusi = getGaussianMax(data[-i])
    PV[i] = n*y.all - (n-1)*yminusi
  }
  return(PV)
}

PVAll = calculatePseudoValues(data)
n = length(PVAll)


jackKnifeEstimate = mean(PVAll)
varJK = sum((PVAll - jackKnifeEstimate)^2)/(n*(n-1))
seJK = sqrt(varJK)

getbootstrapestimate = function(data, sims) {
  theta = numeric(sims)
  varTheta = numeric(sims)
  
  n = length(data)
  index = 1:n
  for (i in 1:sims){
    sampleindex= sample(index,n,replace=TRUE)
    theta[i] = mean(getGaussianMax(data[sampleindex]))
  }
  
  return(list(thetaBS = mean(theta), varBS = var(theta), seBS = sqrt(var(theta))))
}

BS = getbootstrapestimate(data, 100)$seBS 

print(paste("The Mode is ",mean(PVAll) ))

print(paste("The Standard Error of Jackknife Estimator",seJK))
print(paste("The Standard Error of Bootstrap Estimates",BS))



```


9)




```{r,echo=TRUE}


q9data = rnorm(100, 3, 2)

getGausEstimate = function(data){
  d = density(data, kernel="gaussian")
  index = which(d$y == max(d$y), arr.ind =TRUE)
  ans = d$x[index]
  return(ans)
}

calculatePseudoValues = function(data) {
  n = length(data)
  yall = getGausEstimate(data)
  PV = numeric(n)
  for( i in 1:n) {
    yminusi = getGausEstimate(data[-i])
    PV[i] = n*yall - (n-1)*yminusi
  }
  return(PV)
}

# We first use jackknife 
PVAll = calculatePseudoValues(q9data)
n = length(PVAll)
mean(PVAll) 
jackKnifeEstimate = mean(PVAll)
varJK = sum((PVAll - jackKnifeEstimate)^2)/(n*(n-1))
seJK = sqrt(varJK)
seJK

getbootstrapestimate = function(data, nsim) {
  theta = numeric(nsim)
  varTheta = numeric(nsim)
  
  n = length(data)
  index = 1:n
  for (i in 1:nsim){
    sampleindex= sample(index,n,replace=TRUE)
    theta[i] = mean(getGausEstimate(data[sampleindex]))
  }
  
  output = list(thetaBS = mean(theta), varBS = var(theta),
                seBS = sqrt(var(theta)))
  output
}
seBS = getbootstrapestimate(q9data, 100)$seBS 
seBS





```



10)




1 The first step is to sort the values of x and divide the n points into 3 nearly equal groups
2 The summary points (median x, median y) will be estimated  in outer groups R andL
3 We know slope = (yR-yL)/(xR-xL) , Intercept= median(yi-bxi)

Advantages of fitting RR line
1.Robustness
2.Easiest way to obtain linearity of data
3. It is Asymptotically efficient than OLS per step

Disadvantages of  fitting RR line

1. Larger datasets are complex to operate using this
2. We dont arrive at a unique definite solution
3. It operates based on many iterations




11)


1.  Bootstrapping is resamplinng tchnique which helps when there is a doubt in the accutacy of usual distributional assumptions and asymptotic results 
2 . Jackniffing is a special case of bootstrapping. It helps in variance and bias estimation
3 . Bootstrpping  follows subsample data with replacement while the jackniffing resampling follows leave one out and calculate rest approach
4. In understanding variability and bias bootstrapping plays a major role as it is more accurate, the jackniffing helps in understanding bias of a point esstimator
5 Bootstrapping is more reliable when compared to jackniffing.



12)

Given Number of scientific inventions for each year between 1860 and 1959


a) What distribution is appropriate for these count data



```{r,echo=TRUE}
library(e1071) 

library(vcd) 
invent <- c(5,3,0,2,0,3,2,3,6,1,2,1,2,1,3,3,3,5,2,4, 4,0,2,3,7,12,3,10,9,2,3,7,7,2,3,3,6,2,4,3, 5,2,2,4,0,4,2,5,2,3,3,6,5,8,3,6,6,0,5,2, 2,2,6,3,4,4,2,2,4,7,5,3,3,0,2,2,2,1,3,4, 2,2,1,1,1,2,1,4,4,3,2,1,4,1,1,1,0,0,2,0) 
library(kequate)

#Tabulating the given
table(invent)


#histogram

hist(table(invent))


#We see that it is quite similair ot poisson distribution, so we can say that poisson distribution might be appropriate for these count data


#Poisson Plot
distplot(table(invent))


#pLOT OF FREEMAN TUKEY RESIDUALS


ft.res<-FTres(table(invent), sapply(as.array(table(invent)),function(x){(1.142*x-3.94)}))


par(mfrow=c(1,2)) 
plot(ft.res) 
abline(h=c(-2,0,2),lty=c(2,1,2),col=c(2,1,2))
qqnorm(ft.res) 
qqline(ft.res)


# SOme of the points can be seen to deviate from the plot
#Freeman  -Tukey residuals has a normal distribution



```


c
Freeman tukey residuals are given by
 sqrt(ni) +sqrt(ni+1) - sqrt(4mi+1) 
 for a estimated frequency mi and observed frequency ni???????+

The above residuals are used to stabilize the variance in the transformed data. QQ plot shows the normal distribution AND THE FREEMAN TUKEY variables are more clear.






13)
```{r,echo=TRUE}




r1<-c(16,13.6,16.2,14.2,9.3,15.1,10.6,12,11.3,10.5,7.7,10.6)
r2<-c(30.4,27.3,32.4,24.1,27.3,21,19.2,22,19.4,14.9,11.4,18)
r3<-c(34.8,37.1,40.3,30.3,35,38.1,26.2,30.6,25.8,18.1,12.3,17.9)
r4<-c(37.2,41.8,42.1,34.6,38.8,34,30,31.8,27.9,18.9,13,17.9)
r5<-c(35.3,40.6,42.9,32.5,38.6,38.9,30.9,32.4,28.5,19.5,12.5,17.9)
r6<-c(39.2,41.4,43.9,35.4,37.5,39.6,32.4,31.1,28.1,22.2,13.7,18.9)
r7<-c(39.7,44.3,45.5,38.7,42.4,41.4,35.5,31.5,27.8,21.9,14.4,19.9)

df<-rbind(r1,r2,r3,r4,r5,r6,r7)
colnames(df)<-c(111,211,311,412,512,612,721,821,921,1022,1122,1222)
rownames(df)<-c(95,175,250,350,500,675,1000)
results<-medpolish(df)
results


symbolPlot<-function(mat){
  result<-medpolish(mat)
  res<-c(result$residuals)
  genNos<-expand.grid(1:7,1:12)
  plotvar<-cbind(genNos$Var2,genNos$Var1,res)
  pos<-plotvar[plotvar[,3]>=0,]
  max<-sum(abs(pos[,3]))
  symbols(pos[,1],pos[,2],squares = 3*(abs(pos[,3]/(max))),inches = FALSE,xlab="Columns",ylab="Rows",main="Symbol Plot")
  pos<-plotvar[plotvar[,3]<0,]
  symbols(pos[,1],pos[,2],circles = 3*(abs(pos[,3]/(max))),inches = FALSE,add = TRUE)
}



symbolPlot(df)



#The plot shows the data clearly with better visualization of variability in data




#b


AnalogR<- 1-((sum(abs(results$residuals))) /(sum(abs(df-results$overall))))
print(paste("Variability Measure, Analog R Square :",AnalogR))



#c


diag.MP <- function(fit){
  source("rrline.r")
  fit.comp <- matrix(fit$row,ncol=1) %*% matrix(fit$col,nrow=1)/fit$overall
  plot(fit.comp, fit$res,xlab="Comparison value",ylab="Residual",cex=0.5)
  abline(v=0,h=0,lty=2)
  ls <- lm(c(fit$res)~c(fit.comp))
  abline(ls,col="red",lty=3)
  rr <- run.rrline(fit.comp,fit$res,iter=10)
  abline(rr$a, rr$b, col="red")
  pwr1 <- 1 - rr$b
  pwr2 <- 1 - ls$coef[2]
  title("",paste("Approximate power =",format(round(pwr1,2))," or ", format(round(pwr2,2))))
}
diag.MP(results)



#d

df.t <- df^(0.17)
results.t <- medpolish(df.t)
diag.MP(results.t)
analog_r2<- 1-((sum(abs(results.t$residuals))) /(sum(abs(df.t-results.t$overall))))
print(paste("Analog R square after Re-Expression : ",analog_r2))

#e



stem(results.t$residuals,2)


# 5 is an outlier in the above stem leaf plot


#f
boxplot(results.t$residuals, main = "Boxplot of columns")
boxplot(t(results.t$residuals), main = "Boxplot of rows")


#g

forgetitplot <- function(outmpol,outlim=0,...) {
  # outmpol is output of medpolish in library(eda) or library(stats)
  # be sure to assign dimnames to matrix being polished
  oldpar <- par()
  par(fig=c(0,.7,0,1))
  nc <- length(outmpol$col)
  nr <- length(outmpol$row)
  a <- rep(outmpol$row,nc)
  b <- rep(outmpol$col,rep(nr,nc))
  sqrt2 <- sqrt(2)
  ab <- cbind((a-b)/sqrt2,(a+b)/sqrt2)
  xrange <- range(ab[,1]) + c(-.1,.1)*(max(ab[,1])-min(ab[,1]))
  yrange <- range(ab[,2]) + c(-.1,.1)*(max(ab[,2])-min(ab[,2]))
  dx <- (xrange[2]-xrange[1])/50
  dy <- (yrange[2]-yrange[1])/50
  plot(ab[,1],ab[,2],axes=F,xlim=xrange,ylim=yrange,xlab="",ylab="",...)
  segments((min(a)-outmpol$col)/sqrt2, (min(a)+outmpol$col)/sqrt2,
           (max(a)-outmpol$col)/sqrt2, (max(a)+outmpol$col)/sqrt2,lty=3)
  segments((outmpol$row-min(b))/sqrt2, (outmpol$row+min(b))/sqrt2,
           (outmpol$row-max(b))/sqrt2, (outmpol$row+max(b))/sqrt2,lty=3)
  # segments((outmpol$row)/sqrt2-min(b), (outmpol$row)/sqrt2+min(b),
  # (outmpol$row)/sqrt2-max(b), (outmpol$row)/sqrt2+max(b),lty=3)
  yrowloc <- rep(max(b),nr)
  xrowloc <- outmpol$row
  # text((xrowloc-yrowloc)/sqrt2-dx,dy+(xrowloc+yrowloc)/sqrt2,format(1:nr))
  text((xrowloc-yrowloc)/sqrt2-dx,dy+(xrowloc+yrowloc)/sqrt2,
       names(sort(outmpol$row)))
  xcolloc <- rep(max(a),nc)
  ycolloc <- outmpol$col
  # text(dx+(xcolloc-ycolloc)/sqrt2,dy+(xcolloc+ycolloc)/sqrt2,format(1:nc))
  text(dx+(xcolloc-ycolloc)/sqrt2,dy+(xcolloc+ycolloc)/sqrt2,
       names(sort(outmpol$col)))
  ynames <- format(round(outmpol$overall + sqrt2*pretty(ab[,2])))
  axis(2,at=pretty(ab[,2]),labels=ynames)
  # add vertical lines when there is an outlier
  if(abs(outlim) > 1e-4) {
    out.index <- which(abs(outmpol$res) > outlim, arr.ind=T)
    # find (r,c) for outlier indices
    zz.x <- outmpol$row[out.index[,1]]
    zz.y <- outmpol$col[out.index[,2]]
    # outlier points at (zz.x-zz.y)/sqrt2, (zz.x+zz.y)/sqrt2
    # draw segment from here to end of residual
    segments((zz.x-zz.y)/sqrt2, (zz.x+zz.y)/sqrt2,
             (zz.x-zz.y)/sqrt2, (zz.x+zz.y)/sqrt2 + outmpol$res[out.index])
  }
  par <- oldpar
  invisible()
}
forgetitplot(results.t)

# from the plot

#i Factor 95 is more infulential than the others
#ii  At plant 311 Co2 level 1000 had maximum effect



#h


#Bootstrap estimates



bootstrap <- function(mat, R)
{
  row.est <- matrix(0, nrow = R, ncol = nrow(mat))
  col.est <- matrix(0, nrow = R, ncol = ncol(mat))
  overall.est <- c()
  res <- medpolish(mat)
  for(i in 1:R)
  {
    sample.dat <- sample(mat, nrow(mat)*ncol(mat), replace = T)
    samp.mat <- matrix(sample.dat, nrow = nrow(mat), ncol = ncol(mat))
    bs <- medpolish(samp.mat)
    overall.est[i] <- bs$overall
    row.est[i,] <- bs$row
    col.est[i,] <- bs$col
  }
  r.est <- apply(row.est, 2, mean)
  c.est <- apply(col.est, 2, mean)
  
  rsd.est <- apply(row.est, 2, sd)
  csd.est <- apply(col.est, 2, sd)
  
  overall <- mean(overall.est)
  overallsd <- sd(overall.est)
  return(list(row = r.est, col = c.est, overall = overall, rowsd=rsd.est,colsd=csd.est,osd = overallsd ))
}

bs <- bootstrap(df, 1000)

print(bs)







```

